{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cluster Documentation \u00b6 Welcome, browse sections at the top of the site.","title":"Cluster Documentation"},{"location":"#cluster-documentation","text":"Welcome, browse sections at the top of the site.","title":"Cluster Documentation"},{"location":"civic-tech-taxonomy/api/","text":"API \u00b6 The civic-tech-taxonomy \u2018s api-server tool is deployed on this cluster at https://api.taxonomy.sandbox.k8s.brigade.cloud Deployment \u00b6 Merge or push new commits into the master branch in the civic-tech-taxonomy repository After pushing to the master branch, you should see the publish-api-server GitHub Action running or already complete under the Actions tab Once the action finishes, you should be able to confirm that a new tag has been push for the api-server container image matching the SHA of your latest commit to the master branch that it was built from In the nac-sandbox-cluster repository , edit the file civic-tech-taxonomy/api-server.yaml which declares the deployment for the api-server instance hosted on the cluster Find the line specifying the container image path+tag: image : ghcr.io/codeforamerica/civic-tech-taxonomy/api-server:abcdef1 Change the tag portion of the line to match the hash for the one that was just built Commit your change directly to the main branch on the cluster repository if you have access, otherwise open a pull request and ask a cluster administrator to merge it for you. No change will actually get applied to the cluster at this point After the image value has been updated in the main branch in the nac-sandbox-cluster repository, that repository\u2019s prepare-k8s-manifests action will prepare an automated PR for deploying the change to the cluster. Review that PR and ask a cluster administrator to merge it for you. Once the deploy PR is merged, the new version should be online within a few minutes. The deploy process can be monitored via the deploy-k8s-manifests workflow under the Actions tab . After that workflow completes, the new version has been applied to the cluster but the cluster then has to download the new container image and restart the deployment.","title":"API"},{"location":"civic-tech-taxonomy/api/#api","text":"The civic-tech-taxonomy \u2018s api-server tool is deployed on this cluster at https://api.taxonomy.sandbox.k8s.brigade.cloud","title":"API"},{"location":"civic-tech-taxonomy/api/#deployment","text":"Merge or push new commits into the master branch in the civic-tech-taxonomy repository After pushing to the master branch, you should see the publish-api-server GitHub Action running or already complete under the Actions tab Once the action finishes, you should be able to confirm that a new tag has been push for the api-server container image matching the SHA of your latest commit to the master branch that it was built from In the nac-sandbox-cluster repository , edit the file civic-tech-taxonomy/api-server.yaml which declares the deployment for the api-server instance hosted on the cluster Find the line specifying the container image path+tag: image : ghcr.io/codeforamerica/civic-tech-taxonomy/api-server:abcdef1 Change the tag portion of the line to match the hash for the one that was just built Commit your change directly to the main branch on the cluster repository if you have access, otherwise open a pull request and ask a cluster administrator to merge it for you. No change will actually get applied to the cluster at this point After the image value has been updated in the main branch in the nac-sandbox-cluster repository, that repository\u2019s prepare-k8s-manifests action will prepare an automated PR for deploying the change to the cluster. Review that PR and ask a cluster administrator to merge it for you. Once the deploy PR is merged, the new version should be online within a few minutes. The deploy process can be monitored via the deploy-k8s-manifests workflow under the Actions tab . After that workflow completes, the new version has been applied to the cluster but the cluster then has to download the new container image and restart the deployment.","title":"Deployment"},{"location":"civic-tech-taxonomy/mysql/","text":"MySQL \u00b6 An instance of MySQL is hosted to serve a working copy of taxonomy data as loaded by https://github.com/codeforamerica/civic-tech-taxonomy/tree/master/tools/mysql-loader Using portforwarder service account \u00b6 Download the civic-tech-taxonomy-portforwarder.yaml from a teammate into your ~/.kube directory Or, initially using the root KUBECONFIG for the entire cluster, generate a narrowly-scoped KUBECONFIG file for the civic-tech-taxonomy/portforwarder service account: sudo hab pkg install jarvus/mkkubeconfig hab pkg exec jarvus/mkkubeconfig mkkubeconfig civic-tech-taxonomy portforwarder > ~/.kube/civic-tech-taxonomy-portforwarder.yaml Activate the downloaded KUBECONFIG in your current terminal session: export KUBECONFIG = ~/.kube/civic-tech-taxonomy-portforwarder.yaml Get the name of the currently running pods and store them in shell variables: MYSQL_POD = $( kubectl get pod -l service = mysql -o jsonpath = '{.items[0].metadata.name}' ) Forward PostgreSQL port \u00b6 kubectl port-forward \"pods/ $( kubectl get pod -l service = mysql -o jsonpath = '{.items[0].metadata.name}' ) \" 3306 :3306 Database logins Default database username is root and password is currently set in civic-tech-taxonomy/mysql.yaml","title":"MySQL"},{"location":"civic-tech-taxonomy/mysql/#mysql","text":"An instance of MySQL is hosted to serve a working copy of taxonomy data as loaded by https://github.com/codeforamerica/civic-tech-taxonomy/tree/master/tools/mysql-loader","title":"MySQL"},{"location":"civic-tech-taxonomy/mysql/#using-portforwarder-service-account","text":"Download the civic-tech-taxonomy-portforwarder.yaml from a teammate into your ~/.kube directory Or, initially using the root KUBECONFIG for the entire cluster, generate a narrowly-scoped KUBECONFIG file for the civic-tech-taxonomy/portforwarder service account: sudo hab pkg install jarvus/mkkubeconfig hab pkg exec jarvus/mkkubeconfig mkkubeconfig civic-tech-taxonomy portforwarder > ~/.kube/civic-tech-taxonomy-portforwarder.yaml Activate the downloaded KUBECONFIG in your current terminal session: export KUBECONFIG = ~/.kube/civic-tech-taxonomy-portforwarder.yaml Get the name of the currently running pods and store them in shell variables: MYSQL_POD = $( kubectl get pod -l service = mysql -o jsonpath = '{.items[0].metadata.name}' )","title":"Using portforwarder service account"},{"location":"civic-tech-taxonomy/mysql/#forward-postgresql-port","text":"kubectl port-forward \"pods/ $( kubectl get pod -l service = mysql -o jsonpath = '{.items[0].metadata.name}' ) \" 3306 :3306 Database logins Default database username is root and password is currently set in civic-tech-taxonomy/mysql.yaml","title":"Forward PostgreSQL port"},{"location":"development/","text":"Development \u00b6 The Development section provides content covering: Overviews of the internal architecture and components Obtaining development environments Executing development workflows Feature implementation guides and examples","title":"Development"},{"location":"development/#development","text":"The Development section provides content covering: Overviews of the internal architecture and components Obtaining development environments Executing development workflows Feature implementation guides and examples","title":"Development"},{"location":"development/features/ingress/","text":"Ingress \u00b6 Exposing services \u00b6 *.sandbox.k8s.brigade.cloud should be configured to resolve to the cluster\u2019s ingress-nginx service. To route a public hostname to a service in the cluster: Create an Ingress Apply the annotation kubernetes.io/ingress.class: nginx to associate with the cluster\u2019s main ingress service Apply the annotation cert-manager.io/cluster-issuer: letsencrypt-prod to enable automatic setup of an SSL certificate Assign an unused hostname under .sandbox.k8s.brigade.cloud (every public service should start with one of these) Optionally, CNAME a custom hostname to the .sandbox.k8s.brigade.cloud hostname and add it to the same ingress","title":"Ingress"},{"location":"development/features/ingress/#ingress","text":"","title":"Ingress"},{"location":"development/features/ingress/#exposing-services","text":"*.sandbox.k8s.brigade.cloud should be configured to resolve to the cluster\u2019s ingress-nginx service. To route a public hostname to a service in the cluster: Create an Ingress Apply the annotation kubernetes.io/ingress.class: nginx to associate with the cluster\u2019s main ingress service Apply the annotation cert-manager.io/cluster-issuer: letsencrypt-prod to enable automatic setup of an SSL certificate Assign an unused hostname under .sandbox.k8s.brigade.cloud (every public service should start with one of these) Optionally, CNAME a custom hostname to the .sandbox.k8s.brigade.cloud hostname and add it to the same ingress","title":"Exposing services"},{"location":"development/features/sealed-secrets/","text":"Sealed Secrets \u00b6 Prerequisites \u00b6 Configure and deploy a public ingress for sealed-secrets on the cluster Install the kubeseal client command on your local workstation from the latest stable release: https://github.com/bitnami-labs/sealed-secrets/releases Configure public certificate \u00b6 Place the public URL for the target cluster\u2019s sealed secret\u2019s certificate into the SEALED_SECRETS_CERT environment variable: export SEALED_SECRETS_CERT = https://sealed-secrets.sandbox.k8s.brigade.cloud/v1/cert.pem Encrypt secrets to cluster repository \u00b6 Create a Kubernetes Secret manifest containing one or more key+value pair, and then use the kubeseal client to encrypt it into a SealedSecret manifest. The target namespace must be provided and will become part of the encryption such that the secret can only be loaded into that namespace. Commit the sealed secret to the cluster\u2019s repository under the path ${project_namespace}.secrets/ where it will be added to the cluster\u2019s deployed manifests: kubeseal \\ --namespace \"my-project\" \\ -f my-secret.yaml \\ -w ~/Repositories/my-cluster/my-project.secrets/my-secret.yaml","title":"Sealed Secrets"},{"location":"development/features/sealed-secrets/#sealed-secrets","text":"","title":"Sealed Secrets"},{"location":"development/features/sealed-secrets/#prerequisites","text":"Configure and deploy a public ingress for sealed-secrets on the cluster Install the kubeseal client command on your local workstation from the latest stable release: https://github.com/bitnami-labs/sealed-secrets/releases","title":"Prerequisites"},{"location":"development/features/sealed-secrets/#configure-public-certificate","text":"Place the public URL for the target cluster\u2019s sealed secret\u2019s certificate into the SEALED_SECRETS_CERT environment variable: export SEALED_SECRETS_CERT = https://sealed-secrets.sandbox.k8s.brigade.cloud/v1/cert.pem","title":"Configure public certificate"},{"location":"development/features/sealed-secrets/#encrypt-secrets-to-cluster-repository","text":"Create a Kubernetes Secret manifest containing one or more key+value pair, and then use the kubeseal client to encrypt it into a SealedSecret manifest. The target namespace must be provided and will become part of the encryption such that the secret can only be loaded into that namespace. Commit the sealed secret to the cluster\u2019s repository under the path ${project_namespace}.secrets/ where it will be added to the cluster\u2019s deployed manifests: kubeseal \\ --namespace \"my-project\" \\ -f my-secret.yaml \\ -w ~/Repositories/my-cluster/my-project.secrets/my-secret.yaml","title":"Encrypt secrets to cluster repository"},{"location":"getting-started/","text":"Getting Started \u00b6 The Getting Started section provides content covering: Provisioning and initially deploying the cluster Manual setup steps for included services Onboarding new users into workflows","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"The Getting Started section provides content covering: Provisioning and initially deploying the cluster Manual setup steps for included services Onboarding new users into workflows","title":"Getting Started"},{"location":"getting-started/service-setup/grafana/","text":"Grafana \u00b6 New clusters will start with the grafana deployment blocked from starting a pod for lack of the grafana-initial-admin secret existing. Creating this secret will enable grafana to start up and create an initial admin login. The secret should be left on the cluster after that as the deployment requires it, but making changes to it will not update any Grafana login unless Grafana\u2019s persistent storage is reset. Creating grafana-admin-secret \u00b6 Use this command to generate a usable secret with a random password: kubectl -n grafana create secret generic grafana-initial-admin \\ --from-literal = admin-user = admin \\ --from-literal = admin-password = \" $( LC_ALL = C </dev/urandom tr -dc '[:alnum:]_' | head -c 32 ) \"","title":"Grafana"},{"location":"getting-started/service-setup/grafana/#grafana","text":"New clusters will start with the grafana deployment blocked from starting a pod for lack of the grafana-initial-admin secret existing. Creating this secret will enable grafana to start up and create an initial admin login. The secret should be left on the cluster after that as the deployment requires it, but making changes to it will not update any Grafana login unless Grafana\u2019s persistent storage is reset.","title":"Grafana"},{"location":"getting-started/service-setup/grafana/#creating-grafana-admin-secret","text":"Use this command to generate a usable secret with a random password: kubectl -n grafana create secret generic grafana-initial-admin \\ --from-literal = admin-user = admin \\ --from-literal = admin-password = \" $( LC_ALL = C </dev/urandom tr -dc '[:alnum:]_' | head -c 32 ) \"","title":"Creating grafana-admin-secret"},{"location":"getting-started/service-setup/sealed-secrets/","text":"Sealed Secrets \u00b6 The sealed-secrets controller will automatically generate a public+private keypair for each cluster that will be used to encrypt and decrypt secrets. After any new cluster is provisioned, the generated keypair should be backed up externally (i.e. to a shared VaultWarden/BitWarden collection). With the keypair backed up, secrets can be decrypted locally or restored to a new cluster in the future. Back up generated keys \u00b6 From the sealed-secrets README.md : kubectl get secret \\ -n sealed-secrets \\ -l sealedsecrets.bitnami.com/sealed-secrets-key \\ -o yaml \\ > cluster-sealed-secrets-master.key Sensitive data! Be sure to keep this file secure and delete from your working directory after uploading it to a secure credentials vault for backup. Do not commit this file to source control Enable ingress \u00b6 The sealed-secrets helm chart includes an ingress that can be configured to provide a public URL to the cluster\u2019s public certificate that can be used for local kubeseal client operations. To enable the ingress, configure and deploy sealed-secrets/release-values.yaml : sealed-secrets/release-values.yaml ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : letsencrypt-prod hosts : - sealed-secrets.sandbox.k8s.brigade.cloud tls : - secretName : sealed-secrets-tls hosts : - sealed-secrets.sandbox.k8s.brigade.cloud Once deployed, local kubeseal clients can be configured to use it by setting the SEALED_SECRETS_CERT environment variable: export SEALED_SECRETS_CERT = https://sealed-secrets.sandbox.k8s.brigade.cloud/v1/cert.pem","title":"Sealed Secrets"},{"location":"getting-started/service-setup/sealed-secrets/#sealed-secrets","text":"The sealed-secrets controller will automatically generate a public+private keypair for each cluster that will be used to encrypt and decrypt secrets. After any new cluster is provisioned, the generated keypair should be backed up externally (i.e. to a shared VaultWarden/BitWarden collection). With the keypair backed up, secrets can be decrypted locally or restored to a new cluster in the future.","title":"Sealed Secrets"},{"location":"getting-started/service-setup/sealed-secrets/#back-up-generated-keys","text":"From the sealed-secrets README.md : kubectl get secret \\ -n sealed-secrets \\ -l sealedsecrets.bitnami.com/sealed-secrets-key \\ -o yaml \\ > cluster-sealed-secrets-master.key Sensitive data! Be sure to keep this file secure and delete from your working directory after uploading it to a secure credentials vault for backup. Do not commit this file to source control","title":"Back up generated keys"},{"location":"getting-started/service-setup/sealed-secrets/#enable-ingress","text":"The sealed-secrets helm chart includes an ingress that can be configured to provide a public URL to the cluster\u2019s public certificate that can be used for local kubeseal client operations. To enable the ingress, configure and deploy sealed-secrets/release-values.yaml : sealed-secrets/release-values.yaml ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : letsencrypt-prod hosts : - sealed-secrets.sandbox.k8s.brigade.cloud tls : - secretName : sealed-secrets-tls hosts : - sealed-secrets.sandbox.k8s.brigade.cloud Once deployed, local kubeseal clients can be configured to use it by setting the SEALED_SECRETS_CERT environment variable: export SEALED_SECRETS_CERT = https://sealed-secrets.sandbox.k8s.brigade.cloud/v1/cert.pem","title":"Enable ingress"},{"location":"operations/","text":"Operations \u00b6 The Operations section provides content covering: Upgrading cluster components Building hosted environments Maintaining hosted environments Backing up and restoring content Monitoring system health","title":"Operations"},{"location":"operations/#operations","text":"The Operations section provides content covering: Upgrading cluster components Building hosted environments Maintaining hosted environments Backing up and restoring content Monitoring system health","title":"Operations"},{"location":"operations/upgrades/to-0.20/","text":"To 0.20.x \u00b6 cluster-template v0.20.x brings compatibility with Kubernetes 1.22 Upgrade cert-manager APIs \u00b6 Any manifests defining ClusterIssuer objects must be upgraded to the stable API: diff --git a/cert-manager.issuers.yaml b/cert-manager.issuers.yaml index ebd08997..8dba5834 100644 --- a/cert-manager.issuers.yaml +++ b/cert-manager.issuers.yaml @@ -1,4 +1,4 @@ -apiVersion: cert-manager.io/v1alpha2 +apiVersion: cert-manager.io/v1 kind: ClusterIssuer Upgrade cert-manager CRDs \u00b6 If migrating from a pre-1.0 version of cert-manager to a post-1.0 version, you may need to manually delete all CRDs Upgrade sealed-secrets ingress config \u00b6 The newer version of sealed-secrets has a new syntax for configuring its ingress: diff --git a/sealed-secrets/release-values.yaml b/sealed-secrets/release-values.yaml index eb3216c2..3fcef1d3 100644 --- a/sealed-secrets/release-values.yaml +++ b/sealed-secrets/release-values.yaml @@ -6,9 +6,5 @@ ingress: annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-prod - hosts: - - sealed-secrets.sandbox.k8s.example.com - tls: - - secretName: sealed-secrets-tls - hosts: - - sealed-secrets.sandbox.k8s.example.com + hostname: sealed-secrets.sandbox.k8s.example.com + tls: true Upgrade RBAC APIs for any service account manifests \u00b6 diff --git a/admins/project-admin.yaml b/admins/project-admin.yaml index dc5c52d0..b3fedfcc 100644 --- a/admins/project-admin.yaml +++ b/admins/project-admin.yaml @@ -14,7 +14,7 @@ metadata: --- kind: Role -apiVersion: rbac.authorization.k8s.io/v1beta1 +apiVersion: rbac.authorization.k8s.io/v1 metadata: name: deployment-admin Check for deprecated APIs \u00b6 After deploying v0.20.x, check for any locally-defined manifests using deprecated APIs before upgrading the host cluster to Kubernetes v1.22+. Check for deployed objects with deprecated APIs: kubent --target-version 1 .22.0 Check for helm chart snapshots: pluto detect-helm Deployment \u00b6 Before deploying an upgrade to v0.20.x, delete existing ingress-nginx jobs to prevent errors about immutable fields being changed: kubectl -n ingress-nginx delete jobs ingress-nginx-admission-create ingress-nginx-admission-patch For the same reason, also delete the prometheus-kube-state-metrics deployment: kubectl -n prometheus delete deployment prometheus-kube-state-metrics","title":"To 0.20.x"},{"location":"operations/upgrades/to-0.20/#to-020x","text":"cluster-template v0.20.x brings compatibility with Kubernetes 1.22","title":"To 0.20.x"},{"location":"operations/upgrades/to-0.20/#upgrade-cert-manager-apis","text":"Any manifests defining ClusterIssuer objects must be upgraded to the stable API: diff --git a/cert-manager.issuers.yaml b/cert-manager.issuers.yaml index ebd08997..8dba5834 100644 --- a/cert-manager.issuers.yaml +++ b/cert-manager.issuers.yaml @@ -1,4 +1,4 @@ -apiVersion: cert-manager.io/v1alpha2 +apiVersion: cert-manager.io/v1 kind: ClusterIssuer","title":"Upgrade cert-manager APIs"},{"location":"operations/upgrades/to-0.20/#upgrade-cert-manager-crds","text":"If migrating from a pre-1.0 version of cert-manager to a post-1.0 version, you may need to manually delete all CRDs","title":"Upgrade cert-manager CRDs"},{"location":"operations/upgrades/to-0.20/#upgrade-sealed-secrets-ingress-config","text":"The newer version of sealed-secrets has a new syntax for configuring its ingress: diff --git a/sealed-secrets/release-values.yaml b/sealed-secrets/release-values.yaml index eb3216c2..3fcef1d3 100644 --- a/sealed-secrets/release-values.yaml +++ b/sealed-secrets/release-values.yaml @@ -6,9 +6,5 @@ ingress: annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-prod - hosts: - - sealed-secrets.sandbox.k8s.example.com - tls: - - secretName: sealed-secrets-tls - hosts: - - sealed-secrets.sandbox.k8s.example.com + hostname: sealed-secrets.sandbox.k8s.example.com + tls: true","title":"Upgrade sealed-secrets ingress config"},{"location":"operations/upgrades/to-0.20/#upgrade-rbac-apis-for-any-service-account-manifests","text":"diff --git a/admins/project-admin.yaml b/admins/project-admin.yaml index dc5c52d0..b3fedfcc 100644 --- a/admins/project-admin.yaml +++ b/admins/project-admin.yaml @@ -14,7 +14,7 @@ metadata: --- kind: Role -apiVersion: rbac.authorization.k8s.io/v1beta1 +apiVersion: rbac.authorization.k8s.io/v1 metadata: name: deployment-admin","title":"Upgrade RBAC APIs for any service account manifests"},{"location":"operations/upgrades/to-0.20/#check-for-deprecated-apis","text":"After deploying v0.20.x, check for any locally-defined manifests using deprecated APIs before upgrading the host cluster to Kubernetes v1.22+. Check for deployed objects with deprecated APIs: kubent --target-version 1 .22.0 Check for helm chart snapshots: pluto detect-helm","title":"Check for deprecated APIs"},{"location":"operations/upgrades/to-0.20/#deployment","text":"Before deploying an upgrade to v0.20.x, delete existing ingress-nginx jobs to prevent errors about immutable fields being changed: kubectl -n ingress-nginx delete jobs ingress-nginx-admission-create ingress-nginx-admission-patch For the same reason, also delete the prometheus-kube-state-metrics deployment: kubectl -n prometheus delete deployment prometheus-kube-state-metrics","title":"Deployment"}]}