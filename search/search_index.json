{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Civic Cloud Documentation \u00b6 Welcome, browse sections at the top of the site.","title":"Civic Cloud Documentation"},{"location":"#civic-cloud-documentation","text":"Welcome, browse sections at the top of the site.","title":"Civic Cloud Documentation"},{"location":"grafana/","text":"Grafana \u00b6 New clusters will start with the grafana deployment blocked from starting a pod for lack of the grafana-initial-admin secret existing. Creating this secret will enable grafana to start up and create an initial admin login. The secret should be left on the cluster after that as the deployment requires it, but making changes to it will not update any Grafana login unless Grafana\u2019s persistent storage is reset. Creating grafana-admin-secret \u00b6 Use this command to generate a usable secret with a random password: kubectl -n grafana create secret generic grafana-initial-admin \\ --from-literal = admin-user = admin \\ --from-literal = admin-password = \" $( cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1 ) \"","title":"Grafana"},{"location":"grafana/#grafana","text":"New clusters will start with the grafana deployment blocked from starting a pod for lack of the grafana-initial-admin secret existing. Creating this secret will enable grafana to start up and create an initial admin login. The secret should be left on the cluster after that as the deployment requires it, but making changes to it will not update any Grafana login unless Grafana\u2019s persistent storage is reset.","title":"Grafana"},{"location":"grafana/#creating-grafana-admin-secret","text":"Use this command to generate a usable secret with a random password: kubectl -n grafana create secret generic grafana-initial-admin \\ --from-literal = admin-user = admin \\ --from-literal = admin-password = \" $( cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1 ) \"","title":"Creating grafana-admin-secret"},{"location":"ingress/","text":"Ingress \u00b6 *.sandbox.k8s.brigade.cloud should be configured to resolve to the cluster\u2019s ingress-nginx service. To route a public hostname to a service in the cluster: Create an Ingress Apply the annotation kubernetes.io/ingress.class: nginx to associate with the cluster\u2019s main ingress service Apply the annotation cert-manager.io/cluster-issuer: letsencrypt-prod to enable automatic setup of an SSL certificate Assign an unused hostname under .sandbox.k8s.brigade.cloud (every public service should start with one of these) Optionally, CNAME a custom hostname to the .sandbox.k8s.brigade.cloud hostname and add it to the same ingress","title":"Ingress"},{"location":"ingress/#ingress","text":"*.sandbox.k8s.brigade.cloud should be configured to resolve to the cluster\u2019s ingress-nginx service. To route a public hostname to a service in the cluster: Create an Ingress Apply the annotation kubernetes.io/ingress.class: nginx to associate with the cluster\u2019s main ingress service Apply the annotation cert-manager.io/cluster-issuer: letsencrypt-prod to enable automatic setup of an SSL certificate Assign an unused hostname under .sandbox.k8s.brigade.cloud (every public service should start with one of these) Optionally, CNAME a custom hostname to the .sandbox.k8s.brigade.cloud hostname and add it to the same ingress","title":"Ingress"},{"location":"civic-tech-taxonomy/api/","text":"API \u00b6 Deployment \u00b6 Merge or push new commits into the master branch in the civic-tech-taxonomy repository After pushing to the master branch, you should see the publish-api-server GitHub Action running or already complete under the Actions tab Once the action finishes, you should be able to confirm that a new tag has been push for the api-server container image matching the SHA of your latest commit to the master branch that it was built from In the nac-sandbox-cluster repository , edit the file civic-tech-taxonomy/api-server.yaml which declares the deployment for the api-server instance hosted on the cluster 1. Find the line specifying the container image path+tag: 1 2 3 ```yaml image: ghcr.io/codeforamerica/civic-tech-taxonomy/api-server:abcdef1 ``` 1. Change the tag portion of the line to match the hash for the one that was just built 1. Commit your change directly to the main branch on the cluster repository if you have access, otherwise open a pull request and ask a cluster administrator to merge it for you. No change will actually get applied to the cluster at this point 1. After the image value has been updated in the main branch in the nac-sandbox-cluster repository, that repository\u2019s prepare-k8s-manifests action will prepare an automated PR for deploying the change to the cluster. Review that PR and ask a cluster administrator to merge it for you. 1. Once the deploy PR is merged, the new version should be online within a few minutes. The deploy process can be monitored via the deploy-k8s-manifests workflow under the Actions tab . After that workflow completes, the new version has been applied to the cluster but the cluster then has to download the new container image and restart the deployment.","title":"API"},{"location":"civic-tech-taxonomy/api/#api","text":"","title":"API"},{"location":"civic-tech-taxonomy/api/#deployment","text":"Merge or push new commits into the master branch in the civic-tech-taxonomy repository After pushing to the master branch, you should see the publish-api-server GitHub Action running or already complete under the Actions tab Once the action finishes, you should be able to confirm that a new tag has been push for the api-server container image matching the SHA of your latest commit to the master branch that it was built from In the nac-sandbox-cluster repository , edit the file civic-tech-taxonomy/api-server.yaml which declares the deployment for the api-server instance hosted on the cluster 1. Find the line specifying the container image path+tag: 1 2 3 ```yaml image: ghcr.io/codeforamerica/civic-tech-taxonomy/api-server:abcdef1 ``` 1. Change the tag portion of the line to match the hash for the one that was just built 1. Commit your change directly to the main branch on the cluster repository if you have access, otherwise open a pull request and ask a cluster administrator to merge it for you. No change will actually get applied to the cluster at this point 1. After the image value has been updated in the main branch in the nac-sandbox-cluster repository, that repository\u2019s prepare-k8s-manifests action will prepare an automated PR for deploying the change to the cluster. Review that PR and ask a cluster administrator to merge it for you. 1. Once the deploy PR is merged, the new version should be online within a few minutes. The deploy process can be monitored via the deploy-k8s-manifests workflow under the Actions tab . After that workflow completes, the new version has been applied to the cluster but the cluster then has to download the new container image and restart the deployment.","title":"Deployment"},{"location":"civic-tech-taxonomy/mysql/","text":"MySQL \u00b6 An instance of MySQL is hosted to serve a working copy of taxonomy data as loaded by https://github.com/codeforamerica/civic-tech-taxonomy/tree/master/tools/mysql-loader Using portforwarder service account \u00b6 Download the civic-tech-taxonomy-portforwarder.yaml from a teammate into your ~/.kube directory Or, initially using the root KUBECONFIG for the entire cluster, generate a narrowly-scoped KUBECONFIG file for the civic-tech-taxonomy/portforwarder service account: sudo hab pkg install jarvus/mkkubeconfig hab pkg exec jarvus/mkkubeconfig mkkubeconfig civic-tech-taxonomy portforwarder > ~/.kube/civic-tech-taxonomy-portforwarder.yaml Activate the downloaded KUBECONFIG in your current terminal session: export KUBECONFIG = ~/.kube/civic-tech-taxonomy-portforwarder.yaml Get the name of the currently running pods and store them in shell variables: MYSQL_POD = $( kubectl get pod -l service = mysql -o jsonpath = '{.items[0].metadata.name}' ) Forward PostgreSQL port \u00b6 kubectl port-forward \"pods/ $( kubectl get pod -l service = mysql -o jsonpath = '{.items[0].metadata.name}' ) \" 3306 :3306 Database logins Default database username is root and password is currently set in civic-tech-taxonomy/mysql.yaml","title":"MySQL"},{"location":"civic-tech-taxonomy/mysql/#mysql","text":"An instance of MySQL is hosted to serve a working copy of taxonomy data as loaded by https://github.com/codeforamerica/civic-tech-taxonomy/tree/master/tools/mysql-loader","title":"MySQL"},{"location":"civic-tech-taxonomy/mysql/#using-portforwarder-service-account","text":"Download the civic-tech-taxonomy-portforwarder.yaml from a teammate into your ~/.kube directory Or, initially using the root KUBECONFIG for the entire cluster, generate a narrowly-scoped KUBECONFIG file for the civic-tech-taxonomy/portforwarder service account: sudo hab pkg install jarvus/mkkubeconfig hab pkg exec jarvus/mkkubeconfig mkkubeconfig civic-tech-taxonomy portforwarder > ~/.kube/civic-tech-taxonomy-portforwarder.yaml Activate the downloaded KUBECONFIG in your current terminal session: export KUBECONFIG = ~/.kube/civic-tech-taxonomy-portforwarder.yaml Get the name of the currently running pods and store them in shell variables: MYSQL_POD = $( kubectl get pod -l service = mysql -o jsonpath = '{.items[0].metadata.name}' )","title":"Using portforwarder service account"},{"location":"civic-tech-taxonomy/mysql/#forward-postgresql-port","text":"kubectl port-forward \"pods/ $( kubectl get pod -l service = mysql -o jsonpath = '{.items[0].metadata.name}' ) \" 3306 :3306 Database logins Default database username is root and password is currently set in civic-tech-taxonomy/mysql.yaml","title":"Forward PostgreSQL port"}]}